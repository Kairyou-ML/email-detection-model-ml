{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedc28b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è Starting Deep Error Analysis on Best Model (DistilBERT)...\n",
      "   Loading model & tokenizer from models/distilbert-spam...\n",
      "   Loading test data...\n",
      "   Running predictions (this might take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Documents\\PROJECT\\Scientific Writing\\sci-wr\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generating Confusion Matrix...\n",
      "   ‚úÖ Saved heatmap to: reports/confusion_matrix.png\n",
      "\n",
      "üö® FALSE POSITIVES (Safe emails classified as Spam): 1\n",
      "   Why this happens: Aggressive keywords ('free', 'money'), weird formatting, or sarcasm.\n",
      "------------------------------------------------------------\n",
      "üîπ Confidence: 1.00\n",
      "   Text: \"MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "üïµÔ∏è FALSE NEGATIVES (Spam emails classified as Safe): 10\n",
      "   Why this happens: Short text, lack of keywords, or 'conversational' spam.\n",
      "------------------------------------------------------------\n",
      "üî∏ Confidence: 1.00\n",
      "   Text: \"RCT' THNQ Adrian for U text. Rgds Vatian\"\n",
      "------------------------------------------------------------\n",
      "üî∏ Confidence: 1.00\n",
      "   Text: \"ROMCAPspam Everyone around should be responding well to your presence since you are so warm and outgoing. You are bringing in a real breath of sunshine.\"\n",
      "------------------------------------------------------------\n",
      "üî∏ Confidence: 1.00\n",
      "   Text: \"For sale - arsenal dartboard. Good condition but no doubles or trebles!\"\n",
      "------------------------------------------------------------\n",
      "üî∏ Confidence: 1.00\n",
      "   Text: \"Do you realize that in about 40 years, we'll have thousands of old ladies running around with tattoos?\"\n",
      "------------------------------------------------------------\n",
      "üî∏ Confidence: 1.00\n",
      "   Text: \"Latest News! Police station toilet stolen, cops have nothing to go on!\"\n",
      "------------------------------------------------------------\n",
      "\n",
      "üíæ Model & Vectorizer Status\n",
      "   ‚úÖ Best Model (DistilBERT) and Tokenizer are already saved in:\n",
      "      models/distilbert-spam\n",
      "   ready for the Application Phase.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import matplotlib\n",
    "# Force non-interactive backend to avoid Tcl/Tk errors on Windows/Servers\n",
    "matplotlib.use('Agg') \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "# Path to your BEST model (DistilBERT)\n",
    "MODEL_PATH = 'models/distilbert-spam' \n",
    "TEST_DATA_PATH = 'data/test_processed.csv'\n",
    "REPORT_DIR = 'reports/'\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "def analyze_errors():\n",
    "    print(\"üïµÔ∏è Starting Deep Error Analysis on Best Model (DistilBERT)...\")\n",
    "    os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "    \n",
    "    # 1. Load Model & Tokenizer\n",
    "    # ------------------------------------------------\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"‚ùå Error: Model not found at {MODEL_PATH}.\")\n",
    "        print(\"   Make sure you unzipped the best model from the training step.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(f\"   Loading model & tokenizer from {MODEL_PATH}...\")\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load model: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Load Test Data\n",
    "    # ------------------------------------------------\n",
    "    print(\"   Loading test data...\")\n",
    "    if not os.path.exists(TEST_DATA_PATH):\n",
    "        print(f\"‚ùå Error: Test data not found at {TEST_DATA_PATH}.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(TEST_DATA_PATH)\n",
    "    \n",
    "    # Ensure text is string and handle missing values\n",
    "    text_col = 'text' if 'text' in df.columns else 'clean_text'\n",
    "    texts = df[text_col].fillna(\"\").astype(str).tolist()\n",
    "    \n",
    "    # Encode labels (Ham=0, Spam=1)\n",
    "    le = LabelEncoder()\n",
    "    true_labels = le.fit_transform(df['label']) \n",
    "    class_names = le.classes_ # Should be ['ham', 'spam']\n",
    "\n",
    "    # 3. Generate Predictions\n",
    "    # ------------------------------------------------\n",
    "    print(\"   Running predictions (this might take a moment)...\")\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "    dataset = SimpleDataset(encodings)\n",
    "    \n",
    "    trainer = Trainer(model=model)\n",
    "    preds_output = trainer.predict(dataset)\n",
    "    \n",
    "    # Get predicted classes and confidence scores\n",
    "    pred_labels = preds_output.predictions.argmax(-1)\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(preds_output.predictions), dim=-1)\n",
    "    confidence = probs.max(dim=1).values.numpy()\n",
    "\n",
    "    # Add columns to dataframe for analysis\n",
    "    df['true_label'] = true_labels\n",
    "    df['pred_label'] = pred_labels\n",
    "    df['confidence'] = confidence\n",
    "\n",
    "    # 4. Confusion Matrix Heatmap\n",
    "    # ------------------------------------------------\n",
    "    print(\"üìä Generating Confusion Matrix...\")\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix: DistilBERT')\n",
    "    \n",
    "    cm_path = os.path.join(REPORT_DIR, 'confusion_matrix.png')\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "    print(f\"   ‚úÖ Saved heatmap to: {cm_path}\")\n",
    "\n",
    "    # 5. Extract False Positives (Safe -> Marked as Spam)\n",
    "    # ------------------------------------------------\n",
    "    # High Precision is key, so we need to analyze these deeply\n",
    "    fps = df[(df['true_label'] == 0) & (df['pred_label'] == 1)].sort_values(by='confidence', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüö® FALSE POSITIVES (Safe emails classified as Spam): {len(fps)}\")\n",
    "    print(\"   Why this happens: Aggressive keywords ('free', 'money'), weird formatting, or sarcasm.\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, row in fps.head(5).iterrows():\n",
    "        print(f\"üîπ Confidence: {row['confidence']:.2f}\")\n",
    "        print(f\"   Text: \\\"{row[text_col]}\\\"\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # 6. Extract False Negatives (Spam -> Marked as Ham)\n",
    "    # ------------------------------------------------\n",
    "    # These are spam emails that sneaked through\n",
    "    fns = df[(df['true_label'] == 1) & (df['pred_label'] == 0)].sort_values(by='confidence', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüïµÔ∏è FALSE NEGATIVES (Spam emails classified as Safe): {len(fns)}\")\n",
    "    print(\"   Why this happens: Short text, lack of keywords, or 'conversational' spam.\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, row in fns.head(5).iterrows():\n",
    "        print(f\"üî∏ Confidence: {row['confidence']:.2f}\")\n",
    "        print(f\"   Text: \\\"{row[text_col]}\\\"\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # 7. Model Status Check\n",
    "    # ------------------------------------------------\n",
    "    print(\"\\nüíæ Model & Vectorizer Status\")\n",
    "    # For Transformers, the 'Vectorizer' is the 'Tokenizer'\n",
    "    if os.path.exists(os.path.join(MODEL_PATH, 'config.json')) and \\\n",
    "       os.path.exists(os.path.join(MODEL_PATH, 'vocab.txt')):\n",
    "        print(\"   ‚úÖ Best Model (DistilBERT) and Tokenizer are already saved in:\")\n",
    "        print(f\"      {MODEL_PATH}\")\n",
    "        print(\"   ready for the Application Phase.\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Warning: Model files seem incomplete. Please re-run training.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5ecf4",
   "metadata": {},
   "source": [
    "code for kaggle running in gpu t4 x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec843bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Using device: cpu\n",
      "üöÄ Loading data for DistilBERT...\n",
      "   Using column: 'text' for training.\n",
      "Tokenizing data...\n",
      "Initializing DistilBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 173\u001b[39m\n\u001b[32m    170\u001b[39m     tokenizer.save_pretrained(MODEL_DIR)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[43mtrain_distilbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mtrain_distilbert\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    120\u001b[39m model = DistilBertForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mdistilbert-base-uncased\u001b[39m\u001b[33m'\u001b[39m, num_labels=\u001b[32m2\u001b[39m).to(device)\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# 4. Training Arguments\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Output directory\u001b[39;49;00m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Total number of training epochs\u001b[39;49;00m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Batch size\u001b[39;49;00m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# 2e-5 as requested\u001b[39;49;00m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Number of warmup steps\u001b[39;49;00m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Strength of weight decay\u001b[39;49;00m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Directory for storing logs\u001b[39;49;00m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Evaluate every epoch\u001b[39;49;00m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Save checkpoint every epoch\u001b[39;49;00m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Load the best model when finished\u001b[39;49;00m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Disable wandb/mlflow logging\u001b[39;49;00m\n\u001b[32m    138\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# 5. Train\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[32m    142\u001b[39m trainer = CustomTrainer(\n\u001b[32m    143\u001b[39m     model=model,\n\u001b[32m    144\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m    148\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:135\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\Documents\\PROJECT\\Scientific Writing\\sci-wr\\Lib\\site-packages\\transformers\\training_args.py:1811\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1809\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.torchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1814\u001b[39m     warnings.warn(\n\u001b[32m   1815\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`torchdynamo` is deprecated and will be removed in version 5 of ü§ó Transformers. Use\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1816\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_compile_backend` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1817\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1818\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\Documents\\PROJECT\\Scientific Writing\\sci-wr\\Lib\\site-packages\\transformers\\training_args.py:2355\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2351\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2352\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2353\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2354\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\functools.py:1037\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m   1035\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1039\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ADMIN\\Documents\\PROJECT\\Scientific Writing\\sci-wr\\Lib\\site-packages\\transformers\\training_args.py:2225\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2225\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2226\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2227\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2228\u001b[39m         )\n\u001b[32m   2229\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2230\u001b[39m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch import nn\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "# Detect if running on Kaggle to set paths automatically\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    # Try to find the file automatically in input directory\n",
    "    found_train = glob.glob('/kaggle/input/email-processed/train_processed.csv', recursive=True)\n",
    "    found_test = glob.glob('/kaggle/input/email-processed/test_processed.csv', recursive=True)\n",
    "    \n",
    "    if found_train and found_test:\n",
    "        TRAIN_PATH = found_train[0]\n",
    "        TEST_PATH = found_test[0]\n",
    "        print(f\"‚úÖ Auto-detected Kaggle paths:\\n  Train: {TRAIN_PATH}\\n  Test: {TEST_PATH}\")\n",
    "    else:\n",
    "        # Fallback if auto-detection fails\n",
    "        DATA_DIR = '/kaggle/input/email-processed' \n",
    "        TRAIN_PATH = os.path.join(DATA_DIR, 'train_processed.csv')\n",
    "        TEST_PATH = os.path.join(DATA_DIR, 'test_processed.csv')\n",
    "else:\n",
    "    # Local paths\n",
    "    TRAIN_PATH = 'data/train_processed.csv'\n",
    "    TEST_PATH = 'data/test_processed.csv'\n",
    "\n",
    "MODEL_DIR = './results/distilbert-spam' if os.path.exists('/kaggle/working') else 'models/distilbert-spam'\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_LEN = 128     \n",
    "BATCH_SIZE = 16   # Increased to 16 for T4 x2 (Effective batch size will be higher)\n",
    "EPOCHS = 3        \n",
    "LEARNING_RATE = 2e-5 \n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è Using device: {device} (Count: {torch.cuda.device_count()})\")\n",
    "\n",
    "class SpamDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for loading emails.\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Labels are already integers here thanks to LabelEncoder\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Callback to calculate Precision/Recall during training.\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer to handle Class Imbalance (Spam < Ham).\n",
    "    We inject a weighted Loss Function directly into the training loop.\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Calculate Class Weights: Higher weight for Spam (Index 1)\n",
    "        # Weight 6.0 roughly balances a 13% spam / 87% ham ratio\n",
    "        # FIX: Use labels.device instead of model.device. \n",
    "        # In multi-GPU (DataParallel), 'model' is wrapped and doesn't have .device, but inputs are guaranteed to be on the correct GPU.\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 6.0]).to(labels.device))\n",
    "        \n",
    "        # Hardcode num_labels=2 to avoid accessing config from wrapped model\n",
    "        loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def train_distilbert():\n",
    "    # 1. Load Data\n",
    "    # ------------------------------------------------\n",
    "    print(\"üöÄ Loading data for DistilBERT...\")\n",
    "    if not os.path.exists(TRAIN_PATH):\n",
    "        print(f\"‚ùå Error: {TRAIN_PATH} not found.\")\n",
    "        print(\"   If on Kaggle, please upload 'train_processed.csv' and 'test_processed.csv' as a Dataset.\")\n",
    "        return\n",
    "\n",
    "    train_df = pd.read_csv(TRAIN_PATH)\n",
    "    test_df = pd.read_csv(TEST_PATH)\n",
    "    \n",
    "    text_col = 'text' if 'text' in train_df.columns else 'clean_text'\n",
    "    print(f\"   Using column: '{text_col}' for training.\")\n",
    "    \n",
    "    # Handle NaNs just in case\n",
    "    train_texts = train_df[text_col].fillna(\"\").astype(str).tolist()\n",
    "    train_labels_raw = train_df['label'].tolist()\n",
    "    \n",
    "    test_texts = test_df[text_col].fillna(\"\").astype(str).tolist()\n",
    "    test_labels_raw = test_df['label'].tolist()\n",
    "\n",
    "    # --- CRITICAL FIX: Label Encoding (String -> Int) ---\n",
    "    print(\"üîÑ Encoding labels (ham=0, spam=1)...\")\n",
    "    le = LabelEncoder()\n",
    "    train_labels = le.fit_transform(train_labels_raw)\n",
    "    test_labels = le.transform(test_labels_raw) # Use transform to ensure consistency\n",
    "    \n",
    "    # Print mapping to verify\n",
    "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(f\"   Class Mapping: {mapping}\")\n",
    "    \n",
    "    if len(mapping) != 2:\n",
    "        print(\"‚ö†Ô∏è Warning: Detected more than 2 classes. Ensure dataset only has 'spam' and 'ham'.\")\n",
    "\n",
    "    # Split Train into Train/Validation (90/10)\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        train_texts, train_labels, test_size=0.1, random_state=42, stratify=train_labels\n",
    "    )\n",
    "\n",
    "    # 2. Tokenization\n",
    "    # ------------------------------------------------\n",
    "    print(\"üìö Tokenizing data...\")\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LEN)\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_LEN)\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=MAX_LEN)\n",
    "\n",
    "    # Create Datasets\n",
    "    train_dataset = SpamDataset(train_encodings, train_labels)\n",
    "    val_dataset = SpamDataset(val_encodings, val_labels)\n",
    "    test_dataset = SpamDataset(test_encodings, test_labels)\n",
    "\n",
    "    # 3. Initialize Model\n",
    "    # ------------------------------------------------\n",
    "    print(\"üß† Initializing DistilBERT model...\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "    # 4. Training Arguments\n",
    "    # ------------------------------------------------\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          \n",
    "        num_train_epochs=EPOCHS,         \n",
    "        per_device_train_batch_size=BATCH_SIZE,  \n",
    "        per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "        learning_rate=LEARNING_RATE,     \n",
    "        warmup_steps=100,                \n",
    "        weight_decay=0.01,               \n",
    "        logging_dir='./logs',            \n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"epoch\",           \n",
    "        save_strategy=\"epoch\",           \n",
    "        load_best_model_at_end=True,     \n",
    "        report_to=\"none\",\n",
    "        fp16=True,                       # ENABLE Mixed Precision for T4 GPUs (Faster)\n",
    "        dataloader_num_workers=2         # Speed up data loading\n",
    "    )\n",
    "\n",
    "    # 5. Train\n",
    "    # ------------------------------------------------\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüèãÔ∏è Starting Training on {torch.cuda.device_count()} GPU(s)...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 6. Final Evaluation\n",
    "    # ------------------------------------------------\n",
    "    print(\"\\nüß™ Evaluating on Test Set...\")\n",
    "    results = trainer.evaluate(test_dataset)\n",
    "    \n",
    "    print(\"\\nüèÜ DistilBERT Results:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Accuracy:  {results['eval_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {results['eval_precision']:.4f}\")\n",
    "    print(f\"Recall:    {results['eval_recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {results['eval_f1']:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 7. Save Model\n",
    "    # ------------------------------------------------\n",
    "    print(f\"üíæ Saving model to {MODEL_DIR}...\")\n",
    "    model.save_pretrained(MODEL_DIR)\n",
    "    tokenizer.save_pretrained(MODEL_DIR)\n",
    "    \n",
    "    # Zip for easy download from Kaggle\n",
    "    if os.path.exists('/kaggle/working'):\n",
    "        import shutil\n",
    "        shutil.make_archive('distilbert_spam_model', 'zip', MODEL_DIR)\n",
    "        print(\"üì¶ Model zipped to distilbert_spam_model.zip for download.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_distilbert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee92ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa28164",
   "metadata": {},
   "source": [
    "```\n",
    "üöÄ Loading processed data for benchmarking...   \n",
    "Training samples: 4457 | Test samples: 1115üß† \n",
    "Training Naive Bayes...üß† \n",
    "Training SVM (Linear)...\n",
    "üß† Training Random Forest...\n",
    "üèÜ Model Benchmarking Results:------------------------------------------------------------               \n",
    "Accuracy  Precision  Recall  F1-ScoreModel                                               \n",
    "Naive Bayes      0.8511     0.4716  0.9463    0.6295\n",
    "SVM (Linear)     0.9865     0.9786  0.9195    0.9481\n",
    "Random Forest    0.9821     0.9924  0.8725    0.9286\n",
    "------------------------------------------------------------\n",
    "üí° Best Model for Precision (Avoiding False Positives): \n",
    "Random Forest   Precision Score: 0.9924  \n",
    " Saved Random Forest to models/\n",
    " üìä Generating ROC Curves...\n",
    " ‚ö†Ô∏è Model SVM (Linear) does not support probability prediction. Skipping ROC.   \n",
    " üìä ROC Curve saved to: data/roc_curve_comparison.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42c8e9",
   "metadata": {},
   "source": [
    "```\n",
    "‚úÖ Auto-detected Kaggle paths:\n",
    "  Train: /kaggle/input/email-processed/train_processed.csv\n",
    "  Test: /kaggle/input/email-processed/test_processed.csv\n",
    "üñ•Ô∏è Using device: cuda (Count: 2)\n",
    "üöÄ Loading data for DistilBERT...\n",
    "   Using column: 'text' for training.\n",
    "üîÑ Encoding labels (ham=0, spam=1)...\n",
    "   Class Mapping: {'ham': 0, 'spam': 1}\n",
    "üìö Tokenizing data...\n",
    "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "üß† Initializing DistilBERT model...\n",
    "\n",
    "üèãÔ∏è Starting Training on 2 GPU(s)...\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    " [378/378 01:34, Epoch 3/3]\n",
    "Epoch\tTraining Loss\tValidation Loss\tAccuracy\tPrecision\tRecall\tF1\n",
    "1\t0.132900\t0.320092\t0.984305\t0.981818\t0.900000\t0.939130\n",
    "2\t0.166900\t0.558723\t0.982063\t0.964286\t0.900000\t0.931034\n",
    "3\t0.086500\t0.640158\t0.982063\t0.964286\t0.900000\t0.931034\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "\n",
    "üß™ Evaluating on Test Set...\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "To disable this warning, you can either:\n",
    "\t- Avoid using `tokenizers` before the fork if possible\n",
    "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
    "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
    "  warnings.warn(\n",
    " [18/18 00:02]\n",
    "\n",
    "üèÜ DistilBERT Results:\n",
    "------------------------------\n",
    "Accuracy:  0.9901\n",
    "Precision: 0.9929\n",
    "Recall:    0.9329\n",
    "F1-Score:  0.9619\n",
    "------------------------------\n",
    "üíæ Saving model to ./results/distilbert-spam...\n",
    "üì¶ Model zipped to distilbert_spam_model.zip for download.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b1bc1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sci-wr)",
   "language": "python",
   "name": "sci-wr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
